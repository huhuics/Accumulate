# 架构和算法
## 一. 数据分片与路由
**1. 哈希分片(Hash Partition)**    
一种数据分片和路由的通用模型，可以将其看作是一个二级映射关系。第一级映射是key-partition映射，其将数据记录映射到数据分片空间，这往往是多对一的映射关系，即一个数据分片包含多条记录数据；第二级映射是partition-machine映射，其将数据分片映射到物理机中，这一般也是多对一映射关系，即一台物理机容纳多个数据分片。  
+ Round Robin
    - 哈希取模法。H(key) = hash(key) mod K，如果物理机增加1台，则数据和物理机之间的映射关系全被打乱。
    - 该方法缺乏扩展灵活性，原因是该方法将物理机和数据分片两个功能点合二为一，即每台物理机对应一个数据分片，这样key-paitition映射和partition-machine映射也就两位一体。
+ 虚拟桶(Virtual Buckets)
    - 所有记录先通过哈希函数映射到对应的虚拟桶，记录和虚拟桶是多对一的映射关系，即一个虚拟桶包含多条记录信息；第二层映射是虚拟桶和物理机之间的映射关系，同样也是多对一映射，一个物理机可以容纳多个虚拟桶，其具体实现是通过查表实现。
    - 当加入新机器，将某些虚拟桶从原来分配的机器重新分配给新机器，只需修改partition-machine映射表中受影响的个别条目就能实现扩展。
+ 一致性哈希(Consistent Hashing)
    - 将哈希数值空间按照大小组成一个首尾相接的环状序列。对于每台机器，可以根据其ip和端口号经过哈希函数映射到哈希数值空间内，这样不同的机器就成了环状序列中的不同节点，而这台机器则负责存储落在一段有序哈希空间内的数据。
    - 路由问题：沿着有向环顺序查找，效率低；为加快查找速度，可以在每个机器节点配置路由表。
## 二. 数据复制与一致性
**1. 基本原则**    
+ CAP
    - 强一致性(Consisitency)：分布式系统中同一数据多副本情形下，对于数据的更新操作体现出的效果与只有单份数据是一样的
    - 可用性(Availability)：客户端在任何时刻对大规模数据系统的读/写操作都应该保证在限定延时内完成
    - 分区容忍性(Partition Tolerance)：分区间的机器无法进行网络通信的情况
+ BASE原则
    - 基本可用(Basically Available)：大多数情况下系统可用，允许偶尔的失败
    - 软状态或柔性状态(Soft State)：指数据状态不要求在任何时刻都完全保持同步
    - 最终一致性(Eventual Consistency)：一种弱一致性，不要求任意时刻数据保持一致同步，但是要求在给定时间窗口内数据会达到一致状态
    
**2. 副本更新策略**    
+ 同时更新
    - 多副本同时更新
+ 主从式更新
    - 对数据的更新操作首先提交到主副本，再由主副本通知从副本更新
+ 任意节点更新
    - 数据更新请求可能发给多副本中任意一个节点，再由这个节点来负责通知其他副本进行更新  

**3. 一致性协议**    
+ 两阶段提交(Tow-Phrase Commit, 2PC)
    - 表决阶段
    - 提交阶段
    - 如果协调者崩溃，则参与者会存在长时间阻塞的可能
+ 三阶段提交
    - 将2PC的提交阶段再次分为两个阶段：预提交阶段和提交阶段，用于解决2PC长时间阻塞的问题。
    - 实际使用很少，一方面是2PC发生阻塞情况很少；另一方面是3PC效率过低。
    
## 三. 大数据常用数据结构
**1. 布隆过滤器(Bloom Filter)**    
具有很好的空间和时间效率，尤其是空间效率极高，BF常用来检测某个元素是否是巨量数据集合中的成员。    
BF会产生误判，但是不会发生漏判。    

**2. SkipList**    
一种可替代平衡树的数据结构，不像平衡树需要强制保持树的平衡，SkipList依靠随机生成数以一定概率来保持数据的平衡分布，其插入、删除、查找数据时间复杂度都是O(long(N))。    
![](https://github.com/huhuics/Accumulate/blob/master/image/SkipList.jpg)    

**3. LSM树**    
LSM树(Log-structured Merge-tree)的本质是将大量的随机写操作转换成批量的序列写，这可以极大提升磁盘数据写入速度。LSM树非常适合对写操作效率有高要求的应用场景，但是对应付出的代价是读效率有所降低。  

**4. Merkle哈希树(Merkle Hash Tree)**    
![](https://github.com/huhuics/Accumulate/blob/master/image/MerkleHashTree.jpg)    
Merkle树常用来快速侦测部分数据正常或者异常的变动。当某个底层数据发生变化时，其对应Merkle树的子节点哈希值会跟着变化，子节点的父节点哈希值也随之变化，以此类推，直到根节点，其间经过的节点哈希值都发生变化，但是其它无关树节点哈希值并不发生改变。通过Merkle树，可以在O(log(n))时间内快速定位变化的数据内容。    

## 四. 资源调度策略    
**1. FIFO调度策略**    
最简单的资源调度策略，提交的作业按照提交时间先后顺序或者根据优先级次序将其放入线程队列相应位置，在资源调度时按照队列先后顺序，先进先出地进行调度与资源分配。优点是简单；缺点是多用户场景下，新加入的作业很容易出现长时间等待调度的现象。    

**2. 公平调度器(Fair Scheduler)**    
将用户的任务分配到多个资源池(Pool)，每个资源池设定资源分配最低保障和最高限度，也可以指定资源池的优先级，优先级高的资源池会被分配更多的资源，当一个资源池资源有剩余时，可以临时将剩余资源共享给其他资源池。公平调度器的调度过程如下：    
+ 首先，根据每个资源池的最小资源保障量，将系统中的部分资源分配给各个资源池    
+ 其次，根据资源池的指定优先级将剩余资源按照比例分配给各个资源池    
+ 最后，在各个资源池中，按照作业优先级或者根据公平策略将资源分配给各个作业    

**3. 延迟调度策略(Delay Scheduling)**    
准确地说，延迟调度策略不是一个独立的调度方式，往往会作为其他调度策略的辅助措施来增加调度的局部性，以此来增加任务执行效率。其基本思想如下：    
对于当前被调度到要分配资源的任务`i`，如果当前资源不满足数据局部性，那么可以暂时放弃分配公平性，任务`i`不接受当前资源，而是等待后续的资源分配；当前资源可以跳过任务`j`分配给其他待调度任务`j`，如果任务i在被跳过`k`次后任然等不到满足局部性的资源，则放弃数据局部性，被迫接受当前资源来启动任务执行。      

## 五. 分布式协调系统    
**1. Chubby锁服务**    
`Chubby`是理论基础是`Paxos`一致性协议，`Paxos`是在完全分布式环境下，不同客户端能够通过交互通信并投票，对于某个决定达成一致的算法。`Paxos`是完全分布的，没有中心管理节点，需要通过多伦通信和投票来达成最终一致，所以效率较低；`Chubby`出于系统效率考虑，增加了一些中心管理策略。    

**2. ZooKeeper**    
+ 体系结构    
ZooKeeper服务由若干台服务器构成，每台服务器内存中维护相同的类似于文件系统的树形数据结构，其中一台通过ZAB原子广播协议选举作为主控服务器，其他的作为从属服务器。客户端通过TCP协议连接任意一台服务器，如果客户端是读操作请求，则任意一个服务器都可以直接响应请求；如果是更新数据操作，则只能由主控服务器来协调更新操作；如果客户端连接的是从属服务器，则从属服务器会将更新数据请求转发到主控服务器，由其完成更新操作。    

![](https://github.com/huhuics/Accumulate/blob/master/image/ZooKeeper%E6%9E%B6%E6%9E%84%E5%9B%BE.png)    

+ 操作序列化    
客户端通过TCP协议连接，所以可以保证客户端请求的顺序性，同时系统内所有更新操作都需要经过主控服务器，通过这两点主控服务器可以保证更新操作的全局序列性。主控服务器利用ZAB协议将数据更新请求通知所有从属服务器，ZAB保证更新操作的一致性及顺序性。所谓顺序性，是指从属服务器的数据更新顺序和主控服务器的更新顺序是一样的。这个一致性协议采用简单的多数投票仲裁(`Majority Quorums`)方式，这意味着只有多数投票服务器存活，ZooKeeper才能正常运行，即如果有`2f+1`台服务器，最多可以容忍`f`台服务器产生故障。如多数从属服务器向主控服务器确认更新成功，则可以通知客户端本次更新操作成功。    

+ 数据读写    
ZooKeeper的任意一台服务器都可以响应客户端的读操作，这是为何其吞吐量高的主要原因，但这也带来了潜在问题：客户端可能会读到过期数据，因为即使主控服务器已经更新了某个内存数据，但是ZAB协议还未能将其广播到从属服务器。为解决这一问题，ZooKeeper的API方法中提供了`Sync`操作，应用可以根据需要在读数据前调用该操作，其含义是：接收到Sync命令的从属服务器从主控服务器同步状态信息，保证两者完全一致。这样如果在读操作前调用Sync操作，则可以保证客户端一定可以读取到最新状态的数据。    
服务器在响应读/写请求时，都会返回客户端一个渐增的zxid编号，客户端在后续请求中会将这个zxid附带在读/写请求中，这个编号代表了这个服务器目前所见到的更新操作的最高编号。如果一个客户端从某个服务器切换到另外一个服务器，新服务器会保证给这个客户端看到的数据版本不会比之前的服务器数据版本低，这是通过比较客户端发送请求时传来的zxid和服务器本身的最高编号zxid来实现的。如果客户端请求zxid编号高于服务器本身最高zxid编号，说明服务器数据过期，则其从主控服务器同步内存数据到最新状态，然后在响应读操作。    

+ 故障容错    
ZooKeeper通过“重放日志(`Replay Log`)”和“模糊快照(`Fuzzy Snapshot`)”来对服务器故障进行容错。“重放日志”在将更新操作体现在内存数据之前先写入外村日志中避免数据丢失；而“模糊快照”指的是在周期性对内存数据做数据快照时，并不对内存数据加锁，而是用深度遍历的方式将内存中的树形结构转入外存快照数据中，这样就存在着在做数据快照时内存数据可能发生变化而本次快照数据并未体现这一变化的问题，这便是“模糊”的原因。因为ZooKeeper可以保证数据更新操作是幂等的，所以即使“模糊快照”没有体现最新的内存数据状态，但是在服务器故障恢复时，加载进“模糊快照”并根据“重放日志”重新进行一遍操作，系统就会恢复到最新状态。    

+ 数据模型(Data Model)    

![](https://github.com/huhuics/Accumulate/blob/master/image/ZooKeeperDataModel.jpg)    

与Chubby一样，ZooKeeper的内存数据模型类似于传统的文件系统模式，由树形层级目录结构构成，其中节点被称作`Znode`。Znode可以是文件、目录，如果是目录的话还可以有子目录，如果是文件的话，一般需要整体完成读/写操作的小文件，这与Chubby一样是出于避免应用将协调系统当作存储系统来用。    
Znode节点有两种类型：持久节点和临时节点（Chubby也是如此）。持久节点不论客户端会话情况，一直存在，只有当客户端显示调用删除操作才会消失。而临时节点则不同，会在客户端会话结束或者发生故障时被ZooKeeper系统自动清除。客户端可以将节点设置为观察标识(watch)，这样当节点内容发生变化（对目录来说，是其子节点发生变化）时ZooKeeper会通知客户端。另外，节点属性可以设置为自增属性(`Sequential`)，即ZooKeeper会自动将顺序编号赋予节点名字。    

+ 典型应用场景
    - 领导者选举(`Leader Election`)
    - 配置管理(`Configuration Management`)
    - 组成员管理(`Group Membership`)
    - 任务分配
    - 锁管理(`Locks`)
    - 双向路障同步(`Double Barrier`)









