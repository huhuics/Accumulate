# 架构和算法
## 一. 数据分片与路由
**1. 哈希分片(Hash Partition)**    
一种数据分片和路由的通用模型，可以将其看作是一个二级映射关系。第一级映射是key-partition映射，其将数据记录映射到数据分片空间，这往往是多对一的映射关系，即一个数据分片包含多条记录数据；第二级映射是partition-machine映射，其将数据分片映射到物理机中，这一般也是多对一映射关系，即一台物理机容纳多个数据分片。  
+ Round Robin
    - 哈希取模法。H(key) = hash(key) mod K，如果物理机增加1台，则数据和物理机之间的映射关系全被打乱。
    - 该方法缺乏扩展灵活性，原因是该方法将物理机和数据分片两个功能点合二为一，即每台物理机对应一个数据分片，这样key-paitition映射和partition-machine映射也就两位一体。
+ 虚拟桶(Virtual Buckets)
    - 所有记录先通过哈希函数映射到对应的虚拟桶，记录和虚拟桶是多对一的映射关系，即一个虚拟桶包含多条记录信息；第二层映射是虚拟桶和物理机之间的映射关系，同样也是多对一映射，一个物理机可以容纳多个虚拟桶，其具体实现是通过查表实现。
    - 当加入新机器，将某些虚拟桶从原来分配的机器重新分配给新机器，只需修改partition-machine映射表中受影响的个别条目就能实现扩展。
+ 一致性哈希(Consistent Hashing)
    - 将哈希数值空间按照大小组成一个首尾相接的环状序列。对于每台机器，可以根据其ip和端口号经过哈希函数映射到哈希数值空间内，这样不同的机器就成了环状序列中的不同节点，而这台机器则负责存储落在一段有序哈希空间内的数据。
    - 路由问题：沿着有向环顺序查找，效率低；为加快查找速度，可以在每个机器节点配置路由表。
## 二. 数据复制与一致性
**1. 基本原则**    
+ CAP
    - 强一致性(Consisitency)：分布式系统中同一数据多副本情形下，对于数据的更新操作体现出的效果与只有单份数据是一样的
    - 可用性(Availability)：客户端在任何时刻对大规模数据系统的读/写操作都应该保证在限定延时内完成
    - 分区容忍性(Partition Tolerance)：分区间的机器无法进行网络通信的情况
+ BASE原则
    - 基本可用(Basically Available)：大多数情况下系统可用，允许偶尔的失败
    - 软状态或柔性状态(Soft State)：指数据状态不要求在任何时刻都完全保持同步
    - 最终一致性(Eventual Consistency)：一种弱一致性，不要求任意时刻数据保持一致同步，但是要求在给定时间窗口内数据会达到一致状态
    
**2. 副本更新策略**    
+ 同时更新
    - 多副本同时更新
+ 主从式更新
    - 对数据的更新操作首先提交到主副本，再由主副本通知从副本更新
+ 任意节点更新
    - 数据更新请求可能发给多副本中任意一个节点，再由这个节点来负责通知其他副本进行更新  

**3. 一致性协议**    
+ 两阶段提交(Tow-Phrase Commit, 2PC)
    - 表决阶段
    - 提交阶段
    - 如果协调者崩溃，则参与者会存在长时间阻塞的可能
+ 三阶段提交
    - 将2PC的提交阶段再次分为两个阶段：预提交阶段和提交阶段，用于解决2PC长时间阻塞的问题。
    - 实际使用很少，一方面是2PC发生阻塞情况很少；另一方面是3PC效率过低。
    
## 三. 大数据常用数据结构
**1. 布隆过滤器(Bloom Filter)**    
具有很好的空间和时间效率，尤其是空间效率极高，BF常用来检测某个元素是否是巨量数据集合中的成员。    
BF会产生误判，但是不会发生漏判。    

**2. SkipList**    
一种可替代平衡树的数据结构，不像平衡树需要强制保持树的平衡，SkipList依靠随机生成数以一定概率来保持数据的平衡分布，其插入、删除、查找数据时间复杂度都是O(long(N))。    
![](https://github.com/huhuics/Accumulate/blob/master/image/SkipList.jpg)    

**3. LSM树**    
LSM树(Log-structured Merge-tree)的本质是将大量的随机写操作转换成批量的序列写，这可以极大提升磁盘数据写入速度。LSM树非常适合对写操作效率有高要求的应用场景，但是对应付出的代价是读效率有所降低。  

**4. Merkle哈希树(Merkle Hash Tree)**    
![](https://github.com/huhuics/Accumulate/blob/master/image/MerkleHashTree.jpg)    
Merkle树常用来快速侦测部分数据正常或者异常的变动。当某个底层数据发生变化时，其对应Merkle树的子节点哈希值会跟着变化，子节点的父节点哈希值也随之变化，以此类推，直到根节点，其间经过的节点哈希值都发生变化，但是其它无关树节点哈希值并不发生改变。通过Merkle树，可以在O(log(n))时间内快速定位变化的数据内容。    

## 四. 资源调度策略    
**1. FIFO调度策略**    
最简单的资源调度策略，提交的作业按照提交时间先后顺序或者根据优先级次序将其放入线程队列相应位置，在资源调度时按照队列先后顺序，先进先出地进行调度与资源分配。优点是简单；缺点是多用户场景下，新加入的作业很容易出现长时间等待调度的现象。    

**2. 公平调度器(Fair Scheduler)**    
将用户的任务分配到多个资源池(Pool)，每个资源池设定资源分配最低保障和最高限度，也可以指定资源池的优先级，优先级高的资源池会被分配更多的资源，当一个资源池资源有剩余时，可以临时将剩余资源共享给其他资源池。公平调度器的调度过程如下：    
    + 首先，根据每个资源池的最小资源保障量，将系统中的部分资源分配给各个资源池    
    + 其次，根据资源池的指定优先级将剩余资源按照比例分配给各个资源池    
    + 最后，在各个资源池中，按照作业优先级或者根据公平策略将资源分配给各个作业    

**3. 延迟调度策略(Delay Scheduling)**    
准确地说，延迟调度策略不是一个独立的调度方式，往往会作为其他调度策略的辅助措施来增加调度的局部性，以此来增加任务执行效率。其基本思想如下：    
对于当前被调度到要分配资源的任务`i`，如果当前资源不满足数据局部性，那么可以暂时放弃分配公平性，任务`i`不接受当前资源，而是等待后续的资源分配；当前资源可以跳过任务`j`分配给其他待调度任务`j`，如果任务i在被跳过`k`次后任然等不到满足局部性的资源，则放弃数据局部性，被迫接受当前资源来启动任务执行。    







